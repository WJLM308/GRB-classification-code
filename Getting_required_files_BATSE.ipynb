{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a14b531",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from astropy.utils.data import download_file\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import shutil\n",
    "\n",
    "\n",
    "def duration_data_to_df():\n",
    "    \"\"\"\n",
    "    This function makes strips the duration data to only contain the required information and puts it in \"DataFrames\"\n",
    "    \"\"\"\n",
    "    # Create directory\n",
    "    if 'DataFrames' not in os.listdir():\n",
    "        os.mkdir(\"DataFrames\")\n",
    "\n",
    "    # Load and clean DataFrame:\n",
    "    # Messy garbage that adds the names to the table\n",
    "    DF = pd.read_table(\"summary/Duration_BATSE_new.dat\", sep=\"\\s+\", comment='#', header=None, index_col=0)  # Load DataFrame\n",
    "    GRB_Names = pd.read_table(\"summary/Basic_BATSE_new.dat\", sep=\"\\s+\", comment='#', header=None, usecols=[0,1,2],index_col=0)\n",
    "    GRB_Names[1] = GRB_Names[1] + \" \" + GRB_Names[2]\n",
    "    del GRB_Names[2]\n",
    "    DF = pd.concat([GRB_Names, DF], axis=1)\n",
    "    DF.reset_index(inplace=True)\n",
    "    DF.columns = {0,1,2,3,4,5,6,7}\n",
    "    \n",
    "    # print(DF.head())\n",
    "    DF = DF.iloc[:, [0, 1, 2, 4, 5, 7]]  # Only take required columns\n",
    "    DF.columns = ['Trig_id', 'GRBname', 'T50', 'start_T50', 'T90', 'start_T90']  # Name columns\n",
    "    # Convert columns to numeric values\n",
    "    for col in ['T50', 'start_T50', 'T90', 'start_T90']:\n",
    "        # If not possible write Nan\n",
    "        DF[col] = pd.to_numeric(DF[col], errors='coerce')\n",
    "    DF.drop_duplicates(subset='Trig_id', inplace=True)  # Drop duplicate data\n",
    "    DF.set_index('Trig_id', inplace=True, drop=True)  # Set index to GRBname\n",
    "    # Save data\n",
    "    DF.to_pickle(\"DataFrames/duration_data_BATSE.dat\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def fluence_data_to_df():\n",
    "    \"\"\"\n",
    "    Save the best_fit fluence from each GRB and saves it in DataFrames as a pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Load relevant summary files as dataframes\n",
    "    # Formats the fluence data so that it is read correctly -- messy, but it works\n",
    "    df1 = pd.read_table(\"summary/Fluence_BATSE.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 1)\n",
    "    df2 = pd.read_table(\"summary/Fluence_BATSE.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 2)\n",
    "    df3 = pd.read_table(\"summary/Fluence_BATSE.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 3)\n",
    "    df4 = pd.read_table(\"summary/Fluence_BATSE.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 4)\n",
    "    df5 = pd.read_table(\"summary/Fluence_BATSE.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 0)\n",
    "    df_partial_1 = pd.concat([df1, df2, df3, df4, df5], axis=1, ignore_index=True)\n",
    "    \n",
    "    # Adding in the second fluence file\n",
    "    df6 = pd.read_table(\"summary/Fluence_BATSE_cont.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 0)\n",
    "    df7 = pd.read_table(\"summary/Fluence_BATSE_cont.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 1)\n",
    "    df8 = pd.read_table(\"summary/Fluence_BATSE_cont.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 2)\n",
    "    df9 = pd.read_table(\"summary/Fluence_BATSE_cont.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 3)\n",
    "    df10 = pd.read_table(\"summary/Fluence_BATSE_cont.dat\", sep=' ', comment='#', header=None, skipinitialspace=True, skiprows=lambda x: x%5 != 4)\n",
    "    df_partial_2 = pd.concat([df6, df7, df8, df9, df10], axis=1, ignore_index=True)\n",
    "    \n",
    "    df = pd.concat([df_partial_1, df_partial_2], axis=0, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    fluence = pd.concat([df.iloc[:,0], df.iloc[:,1] + df.iloc[:,3] + df.iloc[:,5] + df.iloc[:,7]], axis=1, ignore_index=True)\n",
    "    fluence.columns = [\"Trigger\", \"fluence\"]\n",
    "    fluence.set_index('Trigger', inplace=True, drop=True)\n",
    "    \n",
    "    hardness = pd.concat([df.iloc[:,0], df.iloc[:,3]/df.iloc[:,1]], axis=1, ignore_index=True)\n",
    "    hardness.columns = [\"Trigger\", \"hardness\"]\n",
    "    hardness.set_index('Trigger', inplace=True, drop=True)\n",
    "    hardness = hardness.replace([-np.inf,0,np.inf], np.nan)\n",
    "    \n",
    "    peak_flux = pd.concat([df.iloc[:,0], df.iloc[:,9]], axis=1, ignore_index=True)\n",
    "    peak_flux.columns = [\"Trigger\", \"peak_flux\"]\n",
    "    peak_flux.set_index('Trigger', inplace=True, drop=True)\n",
    "\n",
    "    # Save file\n",
    "    if 'DataFrames' not in os.listdir():\n",
    "        os.mkdir(\"DataFrames\")\n",
    "    \n",
    "    #fluence.to_pickle(\"DataFrames/fluence_data_BATSE.dat\")\n",
    "    hardness.to_pickle(\"DataFrames/hardness_data_BATSE.dat\")\n",
    "    #peak_flux.to_pickle(\"DataFrames/peak_flux_BATSE.dat\")\n",
    "\n",
    "\n",
    "def get_LC(trig):\n",
    "    \"\"\"\n",
    "    Function to download a lightcurve given it's name and trig_id\n",
    "    \"\"\"\n",
    "    \n",
    "    interval = (int(trig)-1)//200*200\n",
    "    lc_url = f\"https://heasarc.gsfc.nasa.gov/FTP/compton/data/batse/trigger/{str(interval+1).zfill(5)}_{str(interval+200).zfill(5)}/{trig.zfill(5)}_burst/discsc_bfits_{trig}.fits.gz\"\n",
    "\n",
    "    try:\n",
    "        tmp_path = download_file(lc_url)\n",
    "        batlc_path = f\"BATSE2/GRB{trig}.fits\"\n",
    "        shutil.move(tmp_path, batlc_path)\n",
    "    except:\n",
    "        print(f\"Download GRB{trig} manually (not automatically downloaded)\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def update_LCs():\n",
    "    \"\"\" Function that downloads the availible light curves. This function will take the duration_data.dat to get list of \n",
    "    trig_ids and GRBnames. \"\"\"\n",
    "\n",
    "    # Make sure the required files are downloaded\n",
    "    #if 'Duration_BATSE.dat' not in os.listdir('summary'):\n",
    "    #    get_summary_files()\n",
    "    if 'duration_data_BATSE.dat' not in os.listdir('DataFrames'):\n",
    "        duration_data_to_df()\n",
    "    if 'BATSE2' not in os.listdir():\n",
    "        os.mkdir(\"BATSE2\")\n",
    "\n",
    "    # Load trig_ids and names from file\n",
    "    trigs = list(pd.read_pickle(\"DataFrames/duration_data_BATSE.dat\").index)\n",
    "\n",
    "    # Already downloaded files\n",
    "    downloaded = list(map(lambda s: s[: -7], os.listdir(\"BATSE2\")))\n",
    "    # print(downloaded)\n",
    "\n",
    "    operations = {'Downloaded': [], 'Error': [], 'Existed': []}\n",
    "    error_log = \"\"\n",
    "\n",
    "    # Loop through names\n",
    "    for trig in trigs:\n",
    "        trig = str(trig)\n",
    "        if trig not in downloaded:  # If not downloaded call function to download\n",
    "            success = get_LC(trig)\n",
    "        else:\n",
    "            # print(f\"{name} is already downloaded\")\n",
    "            operations['Existed'].append(trig)\n",
    "            continue\n",
    "\n",
    "        # Add to log depending on success of it\n",
    "        if success:\n",
    "            # print(f\"{name} downloaded successfully \")\n",
    "            operations['Downloaded'].append(trig)\n",
    "        else:\n",
    "            print(f\"{trig} not downloaded\")\n",
    "            operations['Error'].append(trig)\n",
    "            error_log += f\"{trig} \\t downloading error \\n\"\n",
    "\n",
    "    # Write errors to log\n",
    "    err_file = open(\"Error_log_BATSE2.txt\", \"w\")\n",
    "    err_file.write(error_log)\n",
    "    err_file.close()\n",
    "\n",
    "    # List of operations\n",
    "    return operations\n",
    "\n",
    "    # downloaded = map(lambda s: s[: -7], os.listdir(\"LightCurves\"))\n",
    "    #\n",
    "\n",
    "if __name__ == \"__main__\":  # Make folders if not already in:\n",
    "    if \"summary\" not in os.listdir():\n",
    "        os.mkdir(\"summary\")\n",
    "    if \"DataFrames\" not in os.listdir():\n",
    "        os.mkdir(\"DataFrames\")\n",
    "\n",
    "    # Update the lightcurves\n",
    "    #duration_data_to_df()\n",
    "    fluence_data_to_df()\n",
    "    #log = update_LCs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
