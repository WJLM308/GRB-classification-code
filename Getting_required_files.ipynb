{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a14b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download GRB170131A manually (not automatically downloaded)\n",
      "GRB170131A not downloaded\n",
      "Download GRB160623A manually (trig_id to url)\n",
      "GRB160623A not downloaded\n",
      "Download GRB160409A manually (not automatically downloaded)\n",
      "GRB160409A not downloaded\n",
      "Download GRB150407A manually (not automatically downloaded)\n",
      "GRB150407A not downloaded\n",
      "Download GRB140909A manually (not automatically downloaded)\n",
      "GRB140909A not downloaded\n",
      "Download GRB140611A manually (not automatically downloaded)\n",
      "GRB140611A not downloaded\n",
      "Download GRB131031A manually (not automatically downloaded)\n",
      "GRB131031A not downloaded\n",
      "Download GRB130913A manually (not automatically downloaded)\n",
      "GRB130913A not downloaded\n",
      "Download GRB130518A manually (not automatically downloaded)\n",
      "GRB130518A not downloaded\n",
      "Download GRB120817B manually (not automatically downloaded)\n",
      "GRB120817B not downloaded\n",
      "Download GRB110604A manually (not automatically downloaded)\n",
      "GRB110604A not downloaded\n",
      "Download GRB101204A manually (not automatically downloaded)\n",
      "GRB101204A not downloaded\n",
      "Download GRB090827 manually (not automatically downloaded)\n",
      "GRB090827 not downloaded\n",
      "Download GRB090720A manually (not automatically downloaded)\n",
      "GRB090720A not downloaded\n",
      "Download GRB071112C manually (not automatically downloaded)\n",
      "GRB071112C not downloaded\n",
      "Download GRB071028B manually (not automatically downloaded)\n",
      "GRB071028B not downloaded\n",
      "Download GRB071010C manually (not automatically downloaded)\n",
      "GRB071010C not downloaded\n",
      "Download GRB071006 manually (not automatically downloaded)\n",
      "GRB071006 not downloaded\n",
      "Download GRB070227 manually (not automatically downloaded)\n",
      "GRB070227 not downloaded\n",
      "Download GRB070125 manually (trig_id to url)\n",
      "GRB070125 not downloaded\n",
      "Download GRB060123 manually (trig_id to url)\n",
      "GRB060123 not downloaded\n",
      "Download GRB041219A manually (not automatically downloaded)\n",
      "GRB041219A not downloaded\n"
     ]
    }
   ],
   "source": [
    "'''Most of this should be covered in the notebook'''\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from astropy.utils.data import download_file\n",
    "import shutil\n",
    "\n",
    "\n",
    "def get_summary_files():\n",
    "    \"\"\"\n",
    "    Downloads the required summary files from Swift:\n",
    "    https://swift.gsfc.nasa.gov/results/batgrbcat/index_tables.html\n",
    "    and puts them in a folder called summary_files\n",
    "    \"\"\"\n",
    "    urls = {'Duration': 'https://swift.gsfc.nasa.gov/results/batgrbcat/summary_cflux/summary_general_info/summary_burst_durations.txt',\n",
    "            'Best_fit': 'https://swift.gsfc.nasa.gov/results/batgrbcat/summary_cflux/summary_T100/best_model.txt',\n",
    "            'PL_fluence': 'https://swift.gsfc.nasa.gov/results/batgrbcat/summary_cflux/summary_T100/summary_pow_energy_fluence.txt',\n",
    "            'CPL_fluence': 'https://swift.gsfc.nasa.gov/results/batgrbcat/summary_cflux/summary_T100/summary_cutpow_energy_fluence.txt'\n",
    "            }\n",
    "    if 'summary' not in os.listdir():\n",
    "        os.mkdir(\"summary\")\n",
    "    for sum, url in zip(urls.keys(), urls.values()):\n",
    "        tmp_path = download_file(url)\n",
    "        file_path = \"summary/{}.dat\".format(sum)\n",
    "        shutil.move(tmp_path, file_path)\n",
    "\n",
    "\n",
    "def duration_data_to_df():\n",
    "    \"\"\"\n",
    "    This function makes strips the duration data to only contain the required information and puts it in \"DataFrames\"\n",
    "    \"\"\"\n",
    "    # Create directory\n",
    "    if 'DataFrames' not in os.listdir():\n",
    "        os.mkdir(\"DataFrames\")\n",
    "\n",
    "    # Load and clean DataFrame:\n",
    "    DF = pd.read_table(\"summary/Duration.dat\", sep=\"|\",\n",
    "                       comment='#', header=None, on_bad_lines='skip')  # Load DataFrame\n",
    "    # print(DF.head())\n",
    "    DF = DF.loc[:, [0, 1, 3, 4, 5, 6]]  # Only take required columns\n",
    "    DF.columns = [\"GRBname\", 'Trig_id', 'T90_start',\n",
    "                  'T90_end', 'T100_start', 'T100_end']  # Name columns\n",
    "    # Convert columns to numeric values\n",
    "    for col in ['T90_start', 'T90_end', 'T100_start', 'T100_end']:\n",
    "        # If not possible write Nan\n",
    "        DF[col] = pd.to_numeric(DF[col], errors='coerce')\n",
    "    DF['T90'] = DF.T90_end - DF.T90_start  # Calculate the T90\n",
    "    DF.drop_duplicates(subset='GRBname', inplace=True)  # Drop duplicate data\n",
    "    DF.set_index('GRBname', inplace=True, drop=True)  # Set index to GRBname\n",
    "    DF.index = DF.index.str.strip()  # Strip GRBname for spaces etc.\n",
    "\n",
    "    # Save data\n",
    "    DF.to_pickle(\"DataFrames/duration_data.dat\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def fluence_data_to_df():\n",
    "    \"\"\"\n",
    "    Save the best_fit fluence from each GRB and saves it in DataFrames as a pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Load relevant summary files as dataframes\n",
    "    best_fit = pd.read_table(\n",
    "        \"summary/Best_fit.dat\", comment='#', sep='|', header=None, skipinitialspace=True)\n",
    "    PL_fluence = pd.read_table(\n",
    "        \"summary/PL_fluence.dat\", comment='#', sep='|', header=None, skipinitialspace=True)\n",
    "    CPL_fluence = pd.read_table(\n",
    "        \"summary/CPL_fluence.dat\", comment='#', sep='|', header=None, skipinitialspace=True)\n",
    "\n",
    "    # Set same index: GRBname\n",
    "    for df in [best_fit, PL_fluence, CPL_fluence]:\n",
    "        df.set_index(0, drop=True, inplace=True)\n",
    "        # df.drop_duplicates(subset = 'GRBname', inplace = True) # Drop duplicate data\n",
    "        # DF.set_index('GRBname', inplace = True, drop = True) # Set index to GRBname\n",
    "        df.index = df.index.str.strip()  # Strip GRBname for spaces etc.\n",
    "\n",
    "    # Strip to only total fluence\n",
    "    PL_fluence = PL_fluence.loc[:, 20]\n",
    "    CPL_fluence = CPL_fluence.loc[:, 20]\n",
    "\n",
    "    # Replace PL - fluence with CPL if it is a better fit\n",
    "    CPL_better = best_fit.loc[:, 2].apply(\n",
    "        lambda x: True if x == 'CPL' else False)\n",
    "    fluence = PL_fluence[best_fit.index]\n",
    "    fluence[CPL_better] = CPL_fluence[CPL_better]\n",
    "    \n",
    "    fluence = fluence.reset_index()\n",
    "    fluence.columns = ['GRBname', 'fluence']\n",
    "    fluence = fluence.set_index('GRBname')\n",
    "\n",
    "    # Save file\n",
    "    if 'DataFrames' not in os.listdir():\n",
    "        os.mkdir(\"DataFrames\")\n",
    "    fluence.to_pickle(\"DataFrames/fluence_data.dat\")\n",
    "\n",
    "\n",
    "def get_LC(name, trig_id):\n",
    "    \"\"\"\n",
    "    Function to download a lightcurve given it's name and trig_id\n",
    "    \"\"\"\n",
    "    # Find URL\n",
    "    if len(trig_id) == 6:\n",
    "        lc_url = \"https://swift.gsfc.nasa.gov/results/batgrbcat/%s/data_product/00%s000-results/lc/64ms_lc_ascii.dat\" % (\n",
    "            name, trig_id)\n",
    "    elif len(trig_id) == 11:\n",
    "        lc_url = \"https://swift.gsfc.nasa.gov/results/batgrbcat/%s/data_product/%s-results/lc/64ms_lc_ascii.dat\" % (\n",
    "            name, trig_id)\n",
    "    else:\n",
    "        print('Download %s manually (trig_id to url)' % (name))\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        tmp_path = download_file(lc_url)\n",
    "        batlc_path = \"LightCurves/%s_lc.dat\" % (name)\n",
    "        shutil.move(tmp_path, batlc_path)\n",
    "    except:\n",
    "        print(f\"Download {name} manually (not automatically downloaded)\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def update_LCs():\n",
    "    \"\"\" Function that downloads the availible light curves. This function will take the duration_data.dat to get list of \n",
    "    trig_ids and GRBnames. \"\"\"\n",
    "\n",
    "    # Make sure the required files are downloaded\n",
    "    if 'Duration.dat' not in os.listdir('summary'):\n",
    "        get_summary_files()\n",
    "    if 'duration_data.dat' not in os.listdir('DataFrames'):\n",
    "        duration_data_to_df()\n",
    "    if 'LightCurves' not in os.listdir():\n",
    "        os.mkdir(\"LightCurves\")\n",
    "\n",
    "    # Load trig_ids and names from file\n",
    "    trig_ids = list(pd.read_pickle(\n",
    "        \"DataFrames/duration_data.dat\").loc[:, 'Trig_id'].str.strip())\n",
    "    names = list(pd.read_pickle(\"DataFrames/duration_data.dat\").index)\n",
    "\n",
    "    # Already downloaded files\n",
    "    downloaded = list(map(lambda s: s[: -7], os.listdir(\"LightCurves\")))\n",
    "    # print(downloaded)\n",
    "\n",
    "    operations = {'Downloaded': [], 'Error': [], 'Existed': []}\n",
    "    error_log = \"\"\n",
    "\n",
    "    # Loop through names\n",
    "    for name, trig_id in zip(names, trig_ids):\n",
    "        if name not in downloaded:  # If not downloaded call function to download\n",
    "            success = get_LC(name, trig_id)\n",
    "        else:\n",
    "            # print(f\"{name} is already downloaded\")\n",
    "            operations['Existed'].append(name)\n",
    "            continue\n",
    "\n",
    "        # Add to log depending on success of it\n",
    "        if success:\n",
    "            # print(f\"{name} downloaded successfully \")\n",
    "            operations['Downloaded'].append(name)\n",
    "        else:\n",
    "            print(f\"{name} not downloaded\")\n",
    "            operations['Error'].append(name)\n",
    "            error_log += f\"{name} \\t downloading error \\n\"\n",
    "\n",
    "    # Write errors to log\n",
    "    err_file = open(\"Error_log.txt\", \"w\")\n",
    "    err_file.write(error_log)\n",
    "    err_file.close()\n",
    "\n",
    "    # List of operations\n",
    "    return operations\n",
    "\n",
    "    # downloaded = map(lambda s: s[: -7], os.listdir(\"LightCurves\"))\n",
    "    #\n",
    "\n",
    "if __name__ == \"__main__\":  # Make folders if not already in:\n",
    "    if \"summary\" not in os.listdir():\n",
    "        os.mkdir(\"summary\")\n",
    "    if \"DataFrames\" not in os.listdir():\n",
    "        os.mkdir(\"DataFrames\")\n",
    "\n",
    "    # Update the lightcurves\n",
    "    duration_data_to_df()\n",
    "    fluence_data_to_df()\n",
    "    log = update_LCs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
